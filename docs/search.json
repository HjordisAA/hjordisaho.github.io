[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage!",
    "section": "",
    "text": "Welcome to my portfolio."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My Name is Hjördis Aho, I am a senior at Mount Holyoke College studying statistics and German studies. I am particularly interested in the use of statistical analysis in the field of social justice and political reform.\nConnect with me on LinkedIn @ Hjördis Aho"
  },
  {
    "objectID": "K-Means-Mini-Demo.html",
    "href": "K-Means-Mini-Demo.html",
    "title": "Lab: K-Means",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue.\n\n\n\n\nLoad Packages\nYou likely will need to install some these packages before you can run the code chunk below successfully.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(factoextra)\nlibrary(amap)\n\n\n\nLoad Penguin Data\n\ndata(penguins)\n\n\n\nData Cleaning\n\n# Remove missing values\npenguins&lt;-penguins%&gt;%\n  filter(!is.na(bill_length_mm) & !is.na(bill_depth_mm) & !is.na(species))\n\n# Make data table (named penguins_reduced) that only has\n# bill_length_mm and bill_depth_mm columns\npenguins_reduced &lt;- penguins%&gt;% select(bill_length_mm, bill_depth_mm)\n\n\n\nInitial Visualization\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + \n  geom_point() \n\n\n\n\n\n\n\n\nWe’ll cluster these penguins based on their bill lengths and depths:\n\n\nImplement \\(K\\)-Means\nComplete the code below to run the K-means algorithm using K = 3.\n\nset.seed(244)\n# Run the K-means algorithm\nkmeans_3_round_1 &lt;- kmeans(scale(penguins_reduced), centers = 3) \n    \n# Plot the cluster assignments\npenguins_reduced %&gt;% \n  mutate(kmeans_cluster = as.factor(kmeans_3_round_1$cluster)) %&gt;%\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = kmeans_cluster)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"K-means with K = 3 (round 1)\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhy do we have to set the seed for K-means? In practice, why should we try out a variety of seeds?\n\nAnswer: For predictability. Every time we run the function ‘kmeans’, it initializes the center of the clusters to be random locations. It’s possible that some random locations give better clustering results than others; since k-means is a greedy algorithm its possible for it to have different results each time and is possible for it to get ‘stuck’ on a local solution.\nK-Means Clusters Versus Known Species Groupings\n\n  ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Actual Groupings of Data Based on Species\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nVisually, how well do you think \\(K\\)-means captured the underlying species structure of the data?\n\nAnswer. YOUR ANSWER HERE\n\n\nTuning \\(K\\)\n\nTo implement K-means clustering we must choose an appropriate K! Use the following example to see the two different extreme situations. Typically, the ideal \\(K\\) is somewhere between the two extremes.\nMinimum: \\(K = 2\\) groups/clusters\nMaximum: \\(K = n\\) groups/clusters (one observation per cluster)\n\nWhat happens in the \\(K\\)-means algorithm if \\(K = n\\)?\nAnswer. It initially assigns each observation to its own cluster\nLet’s consider anywhere from \\(K = 2\\) to \\(K = 20\\) clusters.\n\nset.seed(244)\n\nk_2  &lt;- kmeans(scale(penguins_reduced), centers = 2)\nk_20 &lt;- kmeans(scale(penguins_reduced), centers = 20)\n\npenguins_reduced %&gt;% \n  mutate(cluster_2 = as.factor(k_2$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_2)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 2\")\n\n\n\n\n\n\n\npenguins_reduced %&gt;% \n  mutate(cluster_20 = as.factor(k_20$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_20)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 20\") + \n    scale_color_manual(values = rainbow(20))\n\n\n\n\n\n\n\n\nWhat are your general impressions?\nAnswer. YOUR ANSWER HERE\n\n\nFinding Ideal K Value: Silhoutte\n\nThe average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster.\n\nTo do so, it maximizes the distance between clusters and minimizes distance within clusters.\n\nA high average silhouette indicates a good clustering.\nGiven a range of possible K values, the optimal number of clusters (K) is the one that maximizes the average silhouette.\n\nWe can use a built-in silhouette method in the fviz_nbclust function to compute the average silhouette for various K values.\n\nfviz_nbclust(scale(penguins_reduced), kmeans, method='silhouette')\n\n\n\n\n\n\n\n\nBased on the average silhouette approach, what is the optimal \\(K\\) value?\nAnswer. The optimal \\(K\\) value will have the largest average silhouette width, which in this case, is \\(K=3\\).\n\n\nExperimenting with Distance Metrics\nWe can use the Kmeans method (notice the “K” is capitalized in this function name) from the amap library to specify how we are measuring distance in the K-means algorithm.\n\nset.seed(244)\nk_2_manattan = Kmeans(scale(penguins_reduced), centers = 3, \n                      method = \"manhattan\")\nk_2_euclid = Kmeans(scale(penguins_reduced), centers = 3, \n                    method = \"euclidean\")\nk_2_maxnorm = Kmeans(scale(penguins_reduced), centers = 3, \n                     method = \"maximum\")\n\n\n\nfviz_cluster(k_2_euclid, data = scale(penguins_reduced), \n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\nfviz_cluster(k_2_manattan, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\nfviz_cluster(k_2_maxnorm, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\n\nTry changing \\(K\\) to equal 3$ in the code chunk above. How do the clusterings using the 3 distance metrics compare? What do you generally observe?\nAnswer. YOUR ANSWER HERE\nModify the code in the chunk above so that we can easily change the value of K (rather than making sure to change K manually in every line). In general coding practices, is called extracting out a constant."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "STAT 244-SC",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nhsb2=read.csv(\"hsb2.csv\")"
  },
  {
    "objectID": "test.html#the-data",
    "href": "test.html#the-data",
    "title": "STAT 244-SC",
    "section": "The Data",
    "text": "The Data\nVariables in the Dataset\n\nid: Student ID number.\nread: Standardized reading score.\nwrite: Standardized writing score.\nmath: Standardized math score.\nscience: Standardized science score.\nsocst: Standardized social studies score.\ngender: Student’s gender (female or male)\nrace: Student’s race (african american, asian, hispanic, or white)\nses: Socio economic status of student’s family (low,middle, or high)\nschtyp: Type of school (public or private)\nprog: Type of program (general, academic, vocational)\n\n\nVisualization\nPeak at the data\n\nhead(hsb2)\n\n   id gender  race    ses schtyp       prog read write math science socst\n1  70   male white    low public    general   57    52   41      47    57\n2 121 female white middle public vocational   68    59   53      63    61\n3  86   male white   high public    general   44    33   54      58    31\n4 141   male white   high public vocational   63    44   47      53    56\n5 172   male white middle public   academic   47    52   57      53    61\n6 113   male white middle public   academic   44    52   51      63    61\n\n\nCount plot for each of variable of interest\n\nhsb2 %&gt;% ggplot(aes(x = read)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Reading Test Scores', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = write)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Writing Test Scores', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = ses)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Socioeconomic status', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = schtyp)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'School Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = prog)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = gender)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = math)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = science)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = socst)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\n\nGeneralized Pairs Plots\n\nggpairs(hsb2 %&gt;% select(write,read,ses,schtyp,prog))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "STAT 244-SC.html",
    "href": "STAT 244-SC.html",
    "title": "STAT 244-SC",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2"
  },
  {
    "objectID": "STAT 244-SC.html#the-data",
    "href": "STAT 244-SC.html#the-data",
    "title": "STAT 244-SC",
    "section": "The Data",
    "text": "The Data\nVariables in the Dataset\n\nid: Student ID number.\nread: Standardized reading score.\nwrite: Standardized writing score.\nmath: Standardized math score.\nscience: Standardized science score.\nsocst: Standardized social studies score.\ngender: Student’s gender (female or male)\nrace: Student’s race (african american, asian, hispanic, or white)\nses: Socio economic status of student’s family (low,middle, or high)\nschtyp: Type of school (public or private)\nprog: Type of program (general, academic, vocational)\n\n\nVisualization\nPeak at the data\n\nhead(hsb2)\n\n   id gender  race    ses schtyp       prog read write math science socst\n1  70   male white    low public    general   57    52   41      47    57\n2 121 female white middle public vocational   68    59   53      63    61\n3  86   male white   high public    general   44    33   54      58    31\n4 141   male white   high public vocational   63    44   47      53    56\n5 172   male white middle public   academic   47    52   57      53    61\n6 113   male white middle public   academic   44    52   51      63    61\n\n\nCount plot for each of variable of interest\n\nhsb2 %&gt;% ggplot(aes(x = read)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Reading Test Scores', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = write)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Writing Test Scores', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = ses)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Socioeconomic status', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = schtyp)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'School Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = prog)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = gender)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = math)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = science)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = socst)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\n\nGeneralized Pairs Plots\n\nggpairs(hsb2 %&gt;% select(write,read,ses,schtyp,prog))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Test.html",
    "href": "Test.html",
    "title": "STAT 244-SC (Sp 25)",
    "section": "",
    "text": "In this study, we examined a set of potential factors that may influence high school students’ standardized test scores. Our goal was to better understand how certain demographic characteristics and academic performance measures relate to one another—particularly how they may predict writing scores. Specifically, we explore the predictive power of familial socioeconomic status, types of school attended, types of program enrolled in, and standardized reading scores on the standardized writing scores of high school students. To assess the relative importance and predictive accuracy of these variables, we conducted k-fold cross validation to compare the performance of two linear models alongside variable selection. This approach allowed us to evaluate model generalizability and identify which factors most consistently contribute to writing performance. Our findings from this analysis may offer insights into patterns of educational achievement among High School students and contribute to larger discussions about equity in education.\n\n\nThe data used in this project comes from the hsb2 dataset, a random sample of size 200 published by UCLA Institute for Digital Research & Education Statistical Consulting. The sample was drawn from the 1,100 high school students who participated in the “High School and Beyond” survey conducted in 1980 by the National Center for Education Statistics. The purpose of this survey was to better understand what factors of the educational system influence the educational and occupational performance of students before and after graduation. The dataset includes variables such as gender, race, socioeconomic status, and standardized test scores in reading, writing, math, science, and social studies. For this analysis, we focused on four predictors: socioeconomic status, type of school attended (public or private), type of academic program enrolled in (general, academic, or vocations), and reading scores, and one response variable, writing scores.\n\nhsb2=read.csv(\"hsb2.csv\")\nhead(hsb2)\n\n   id gender  race    ses schtyp       prog read write math science socst\n1  70   male white    low public    general   57    52   41      47    57\n2 121 female white middle public vocational   68    59   53      63    61\n3  86   male white   high public    general   44    33   54      58    31\n4 141   male white   high public vocational   63    44   47      53    56\n5 172   male white middle public   academic   47    52   57      53    61\n6 113   male white middle public   academic   44    52   51      63    61\n\n\n\n\nData cleaning was the first step of our study. All variable names were easy to use incode and all quantitative variables had the correct type, thus none of these variables needed to be changed. However, when we checked the variable type for our categorical variables, we saw they were type ‘character’; we then changed them to type factor.\n\n#Checking our quantitative variable types\nsapply(hsb2[,c(\"read\",\"write\")], class)\n\n     read     write \n\"integer\" \"integer\" \n\n#Checking our cateforial variable types\nsapply(hsb2[,c (\"ses\",\"schtyp\",\"prog\")], class)\n\n        ses      schtyp        prog \n\"character\" \"character\" \"character\" \n\n#Changing them to type factor\nhsb2$ses = as.factor(hsb2$ses)\nhsb2$schtyp = as.factor(hsb2$schtyp)\nhsb2$prog = as.factor(hsb2$prog )\nhsb2$gender=as.factor(hsb2$gender)\n\nIn order to look for missing values, we counted the original number of rows in our dataset and then counted again after filtering all NAs from the data. We started off with 200 rows and after filtering the dataset, 200 rows remained, thus the data contained no missing values.\n\nnrow(hsb2) #checking our starting number of rows\n\n[1] 200\n\n# Removing missing data points\nhsb2= hsb2 %&gt;% filter(!is.na(read), !is.na(write),!is.na(ses),!is.na(schtyp),!is.na(prog))\nnrow(hsb2)\n\n[1] 200\n\nhsb2 %&gt;% ggplot(aes(x = read)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Reading Test Scores', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = write)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Writing Test Scores', y = 'Count')\n\n\n\n\n\n\n\n# Checking for any empty strings\nhsb2 %&gt;% ggplot(aes(x = ses)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Socioeconomic status', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = schtyp)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'School Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = prog)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\n\n\n\n\nNumerical Summaries\nOur research question we are trying to find is how are writing scores affected by multiple variables of the high school senior. The variables we are going to look at is reading scores, socioeconomic status, school type, and program type.\nLet’s first review our quantitative variables. For our standardized writing scores, our mean score is around 52.77 with range from 31 to 67. Our standard deviation is rounded to 9.5. For our standardized reading scores, our mean is 52.23, with range from 28 to 76. Our standard deviation for these scores is rounded to 10.3. We have 200 observations.\n\nsummary(hsb2$write)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  31.00   45.75   54.00   52.77   60.00   67.00 \n\nsd(hsb2$write)\n\n[1] 9.478586\n\nsummary(hsb2$read)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.00   44.00   50.00   52.23   60.00   76.00 \n\nsd(hsb2$read)\n\n[1] 10.25294\n\n\nIn terms of categorical variables, we can see that for socioeconomic status for students, the majority of them, 47.5%, are in the middle class. For school type, most of the senior students, 84%, are in public school. And finally for program type, most seniors, 52.5% are in academic program.\n\nhsb2 %&gt;% count(ses) %&gt;% mutate(percentage = n/sum(n)*100)\n\n     ses  n percentage\n1   high 58       29.0\n2    low 47       23.5\n3 middle 95       47.5\n\nhsb2 %&gt;% count(schtyp) %&gt;% mutate(percentage = n/sum(n)*100)\n\n   schtyp   n percentage\n1 private  32         16\n2  public 168         84\n\nhsb2 %&gt;% count(prog) %&gt;% mutate(percentage = n/sum(n)*100)\n\n        prog   n percentage\n1   academic 105       52.5\n2    general  45       22.5\n3 vocational  50       25.0\n\n\nVisual Summary\nWe have already seen some plots of our data in our data cleaning section but I will also be providing a bit more to visualize our data and the relationships between each other. Starting with our response variable, standardized writing scores.\n\nggplot(hsb2, aes(y=write)) + geom_boxplot(fill = \"lightblue\") + labs(title = \"Standardized Writing Scores\", y = \"Count\")+ theme_classic()\n\n\n\n\n\n\n\n\nEarlier in the data cleaning, we showed data visualization for each variable, now let us compare them to our response variable.\n\nggplot(hsb2, aes(x = ses,y=write)) + geom_boxplot(fill = \"lightblue\") + labs( y= \"Writing Score\", x = \"Socioeconomic Status\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = schtyp,y= write)) + geom_boxplot(fill = \"lightblue\") + labs(y = \"Writing Score\", x = \"School Type\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = prog,y= write)) + geom_boxplot(fill = \"lightblue\") + labs(y = \"Writing Score\", x = \"Program Type\")+ theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first statistical test we conducted was K-fold cross validation. Cross validation is a frequently used technique for evaluating the performance of predictive models by assessing how well they are able to generalize to data that is independent of the training set. In K-fold cross validation, this is done through randomly partitioning the dataset into k approximately equal-sized groups or folds. The model is trained on k-1 folds and then tested on the remaining fold. This process is repeated k times so that each fold serves as the test set exactly one time. Mathematically, K-folds cross validation follows this procedure,\n\nDivide the data into k partitions of approximately equal size.\nRepeated the following for each group j (j=1,2,…,k)\nassign group j to the test set.\n\nFit a model on the remaining k-1 partitions or the training set.\nUse this model to predict the response for all observations in group j.\nCalculate the needed error metric for group j. In our case we used the MSE\n\nCombine this information to estimate model quality\n\n\\[CV_{(k)}=\\frac{1}{k}\\sum_{j=1}^{k}MSE_J\\]\nIn our study, we use the MSE as our error metric. The MSE penalizes larger errors more heavily than other metrics. This is important for our data because we are predicting test scores and thus making large errors in our prediction can have large impacts. One large disadvantage of using the MSE would be that it is sensitive to outliers. Since there may be some students whose test scores are considered outliers, this may influence the MSE more heavily than other metrics.\n\\(MSE=\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=(RMSE)^2\\)\nThe goal of cross validation is to estimate how well a model will perform on independent datasets to prevent overfitting. Overfitting can cause harmful biases that may be causing patterns in the training data to be generalized to future data, ultimately further perpetuating inaccurate, and sometimes dangerous information. This is especially important when conducting educational research as overfitting can lead to the spread of misleading and sometimes dangerous information that could impact a large proportion of the younger population. Cross validation provides more fair and accurate prediction of the population of interest.\nIn our study, we analysed the CV MSE of the following models using three different values of k. The models were as follows,\nModel 1: ‘write’ ~ ‘gender’ + ‘ses’ + ‘schtyp’ + ‘prog’ + ‘read’ + ’ math’ + ‘science’ + ‘socst’\nModel 2: ‘write’ ~ ‘ses’ + ’ schtyp’ + ‘prog’ + ‘read’\nFirst, we implemented a 10-fold cross validation,\n\nset.seed(24667)\nlm_spec=linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"lm\")\n\n# Model Estimation\nmodel1=lm_spec %&gt;%\n  fit(write~gender+ses+schtyp+prog+read+math+science+socst, data=hsb2)\n\nmodel2=lm_spec %&gt;%\n  fit(write~ses+schtyp+prog+read, data=hsb2)\n\n\nset.seed(9359)\nmodel1_cv=lm_spec %&gt;%\n  fit_resamples(\n    write~gender+ses+schtyp+prog+read+math+science+socst,\n    resamples=vfold_cv(hsb2,v=10),\n    metrics=metric_set(mae, rmse, rsq)\n  )\n\n\nmodel2_cv=lm_spec %&gt;%\n  fit_resamples(\n    write~ses+schtyp+prog+read,\n    resamples=vfold_cv(hsb2,v=10),\n    metrics=metric_set(mae,rmse,rsq)\n  )\n\n\nIn order to retrieve the MSE for all folds in our cross validation we must first unnest the cross validation to obtain each individual fold and the RMSE. We then squared the RMSE to obtain the MSE for each individual fold, and following the k-fold cross validation procedure, averaged the MSE for each model. These MSE values are what is printed below.\n\n\n# Get fold by fold and pull out the rmse for each. \nm1_rmse=model1_cv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\nm2_rmse=model2_cv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\n# Sqaure the found RMSE values\nm1_mse=m1_rmse$.estimate^2\n\nm2_mse=m2_rmse$.estimate^2\n\n# Average the MSE values across the 10 folds for each model\nm1_cv_mse=(1/10)*sum(m1_mse)\nm2_cv_mse=(1/10)*sum(m2_mse)\n\n# Print the CV MSE for both models. \nm1_cv_mse\n\n[1] 40.57728\n\nm2_cv_mse\n\n[1] 58.62453\n\n\n\n# Summarize RMSE for model 1\nm1_summary &lt;- m1_rmse %&gt;%\n  summarise(\n    model = \"Model 1\",\n    mean_rmse = mean(.estimate),\n    sd_rmse = sd(.estimate),\n    mean_mse = mean(.estimate^2),\n    sd_mse = sd(.estimate^2)\n  )\n\n# Summarize RMSE for model 2\nm2_summary &lt;- m2_rmse %&gt;%\n  summarise(\n    model = \"Model 2\",\n    mean_rmse = mean(.estimate),\n    sd_rmse = sd(.estimate),\n    mean_mse = mean(.estimate^2),\n    sd_mse = sd(.estimate^2)\n  )\n\n# Combine into one summary table\nmodel_metrics_summary &lt;- bind_rows(m1_summary, m2_summary)\n\n# Display the table\nmodel_metrics_summary\n\n# A tibble: 2 × 5\n  model   mean_rmse sd_rmse mean_mse sd_mse\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Model 1      6.30   0.986     40.6   12.2\n2 Model 2      7.60   0.968     58.6   14.9\n\n\n\nWe found that the CV MSE for model 1 is, 40.577 while the CV MSE for model 2 is 58.625.\n\nNext, we conducted a leave one out cross validation of the two models.\n\nset.seed(1376987)\n\nk=nrow(hsb2)-1\n\n# Performing K=199 CV\nmodel1_loocv = lm_spec %&gt;%\n  fit_resamples(\n    write ~ gender + ses + schtyp + prog + read + math + science + socst,\n    resamples = vfold_cv(hsb2,v=k),\n    metrics = metric_set(mae, rmse)\n  )\n\nmodel2_loocv = lm_spec %&gt;%\n  fit_resamples(\n    write ~ ses + schtyp + prog + read,\n    resamples = vfold_cv(hsb2,v=k),\n    metrics = metric_set(mae, rmse)\n  )\n\n# Calculated the CV MSE for model 1 \n\nm1_loo_rmse=model1_loocv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\nm1_loo_mse=m1_loo_rmse$.estimate^2\nm1_loo_mse=(1/k)*sum(m1_loo_mse)\n\n\n# Calulate the CV MSE for model 2\n\nm2_loo_rmse=model2_loocv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n \nm2_loo_mse=m2_loo_rmse$.estimate^2\nm2_loo_mse=(1/k)*sum(m2_loo_mse)\n\n\n# Print the CV MSE for both models. \n(m1_loo_mse)\n\n[1] 39.32325\n\n(m2_loo_mse)\n\n[1] 57.82414\n\n\n\nWe can see that the 199-Fold CV MSE for Model 1 is 39.323, and the 199-fold CV MSE for Model 2 is 57.824. This shows that Model 1 has the lowest CV MSE when k=199.\n\nFinally, we implemented a 75-fold Cross Validation. Since we tested K=10 and K=199, this provides information into the CV-MSE when k is between the two previously found values.\n\nset.seed(239762)\n# Performing k=75 CV on both models\n\nmodel1_75 = lm_spec %&gt;%\n  fit_resamples(\n    write ~ gender + ses + schtyp + prog + read + math + science + socst,\n    resamples = vfold_cv(hsb2,v=75),\n    metrics = metric_set(mae, rmse, rsq)\n  )\n\nmodel2_75 = lm_spec %&gt;%\n  fit_resamples(\n    write ~ ses + schtyp + prog + read,\n    resamples = vfold_cv(hsb2,v=75),\n    metrics = metric_set(mae, rmse, rsq)\n  )\n\n# Calculated the CV MSE for model 1 \n\nm1_75_rmse=model1_75 %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\nm1_75_mse=m1_75_rmse$.estimate^2\nm1_75_mse=(1/75)*sum(m1_75_mse)\n\n\n# Calulate the CV MSE for model 2\n\nm2_75_rmse=model2_75 %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n \nm2_75_mse=m2_75_rmse$.estimate^2\nm2_75_mse=(1/75)*sum(m2_75_mse)\n\n\n# Print the CV MSE for both models. \n(m1_75_mse)\n\n[1] 39.88911\n\n(m2_75_mse)\n\n[1] 57.52963\n\n\n\nWe find that the 75-Fold CV MSE for model 1 is, 39.889, while the 75-Fold CV MSE for model 2 is 57.530.\n\nNotice that for Model 1, when k=199, the CV error is the smallest. For model 2, we notice that the CV error is smallest for k=75. Both models seem to perform better when K is larger than 10. Based on the all three calculated CV tests, we see a clearly that the CV MSE for Model 1 is consistently lower than that of Model 2. Due to this persistent results, we will select Model 1 as our final model.\nModel 1: ‘write’ ~ ‘gender’ + ‘ses’ + ‘schtyp’ + ‘prog’ + ‘read’ + ’ math’ + ‘science’ + ‘socst’\n\n\n\nNow, we used backward stepwise selection to select our model and which variables are efficient for us. We build a model with all p possible predictors and then we repeat the following until only one predictor remains:\n\nRemove the 1 predictor that increases the MSE/MAE by the least\nBuild a model with the remaining predictors.\n\nWe then pick the best model. This method helps us balance accuracy and ensuring we avoid overfitting.\n\n#Model\nlm_spec = linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% set_mode(\"regression\")\n#Full Model \nlm_spec %&gt;% fit(write ~ gender + ses + schtyp + prog + read + math + science + socst, data = hsb2) %&gt;% tidy() %&gt;% mutate (p.value = round(p.value, 4))\n\n# A tibble: 11 × 5\n   term           estimate std.error statistic p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      15.0      3.85       3.89   0.0001\n 2 gendermale       -5.37     0.889     -6.04   0     \n 3 seslow            0.851    1.31       0.648  0.518 \n 4 sesmiddle        -0.149    1.06      -0.141  0.888 \n 5 schtyppublic     -1.07     1.22      -0.877  0.382 \n 6 proggeneral      -1.07     1.19      -0.896  0.371 \n 7 progvocational   -1.74     1.27      -1.36   0.174 \n 8 read              0.114    0.0657     1.74   0.0841\n 9 math              0.207    0.0704     2.94   0.0037\n10 science           0.260    0.0628     4.14   0.0001\n11 socst             0.216    0.0556     3.90   0.0001\n\n#Model Selection \nmodel_8predictors = lm_spec %&gt;% fit_resamples(write~ gender + ses + schtyp + prog + read + math + science + socst, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_7predictors = lm_spec %&gt;% fit_resamples(write~ gender + ses + schtyp + prog + read + math + socst, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_6predictors = lm_spec %&gt;% fit_resamples(write~ ses + schtyp + prog + read + math + socst, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_5predictors = lm_spec %&gt;% fit_resamples(write ~ ses + schtyp + prog + read + math, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_4predictors = lm_spec %&gt;% fit_resamples(write ~ ses + schtyp + prog + read, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_3predictors = lm_spec %&gt;% fit_resamples(write ~ ses + prog + read, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_2predictors = lm_spec %&gt;% fit_resamples(write ~ ses + read, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_1predictors = lm_spec %&gt;% fit_resamples(write ~ read, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\n\n#Results\nresults = bind_rows( model_1predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 1),model_2predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 2),model_3predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 3),model_4predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 4),model_5predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 5),model_6predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 6),model_7predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 7),model_8predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 8))\n\n#Plot them out\nggplot(results %&gt;% filter(.metric == \"mae\") , aes(x = predictors, y = mean)) + geom_point(color = \"lightblue\") + geom_line(color = \"lightblue\") + theme_classic()+ labs(title = \"MAE vs Num of Predictors\", x =\" Num of Predictors\", y=\"MAE\")\n\n\n\n\n\n\n\n\nBased on our graph, we can see that our lowest MAE is with all eight of our predictors. As we continued to remove a predictor each time, our MAE increased which means that all variable are significant and contribute to our model and response variable.\n\n\n\n\nAfter our K-Fold Cross-Validation method and backward stepwise selection method, they gave us the both results and we determined that Model 1 is the best. Model 1, includes all eight predictors (gender, ses, schtyp, prog, read, math, science, socst) had the lowest MAE, mean absolute error, with both methods.\nModel 1 is able to provided the most significant model for standardized writing scores as it shows a comprehensive view of the student’s academic and demographic statistics. They all contribute to a students’ writing scores which is what we are testing. We originally picked the four predictive variables (ses, schtyp, prog, and read) because we believed those were the only significant ones.\nHowever, through statistical analysis, we saw that additional variables (math, science, gender) help explain variance in the standardized writing scores. To conclude, through analysis we cannot accurately predict the student’s writing performance by only a subset of variables. Based on our methods, we need all variables which lets us know the students’ full demographics and academic statistics, which helps us accurately predict student’s writing scores to the best we can.\n\n\n\nNational Center for Education Statistics - Statistical Consulting. (n.d.). High School & Beyond (HS&B) - overview. High School & Beyond (HS&B) - Overview. https://nces.ed.gov/surveys/hsb/\nExploring Data with Graphic; R Learning Modules. UCLA: Statistical Consulting Group. from https://stats.oarc.ucla.edu/r/modules/exploring-data-with-graphics/ (accessed April 28, 2025)."
  },
  {
    "objectID": "Test.html#the-data",
    "href": "Test.html#the-data",
    "title": "STAT 244-SC",
    "section": "",
    "text": "Variables in the Dataset\n\nid: Student ID number.\nread: Standardized reading score.\nwrite: Standardized writing score.\nmath: Standardized math score.\nscience: Standardized science score.\nsocst: Standardized social studies score.\ngender: Student’s gender (female or male)\nrace: Student’s race (african american, asian, hispanic, or white)\nses: Socio economic status of student’s family (low,middle, or high)\nschtyp: Type of school (public or private)\nprog: Type of program (general, academic, vocational)\n\n\n\nPeak at the data\n\nhead(hsb2)\n\n   id gender  race    ses schtyp       prog read write math science socst\n1  70   male white    low public    general   57    52   41      47    57\n2 121 female white middle public vocational   68    59   53      63    61\n3  86   male white   high public    general   44    33   54      58    31\n4 141   male white   high public vocational   63    44   47      53    56\n5 172   male white middle public   academic   47    52   57      53    61\n6 113   male white middle public   academic   44    52   51      63    61\n\n\nCount plot for each of variable of interest\n\nhsb2 %&gt;% ggplot(aes(x = read)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Reading Test Scores', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = write)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Writing Test Scores', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = ses)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Socioeconomic status', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = schtyp)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'School Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = prog)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = gender)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = math)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = science)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = socst)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\n\nGeneralized Pairs Plots\n\nggpairs(hsb2 %&gt;% select(write,read,ses,schtyp,prog))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Test.html#data",
    "href": "Test.html#data",
    "title": "STAT 244-SC (Sp 25)",
    "section": "",
    "text": "The data used in this project comes from the hsb2 dataset, a random sample of size 200 published by UCLA Institute for Digital Research & Education Statistical Consulting. The sample was drawn from the 1,100 high school students who participated in the “High School and Beyond” survey conducted in 1980 by the National Center for Education Statistics. The purpose of this survey was to better understand what factors of the educational system influence the educational and occupational performance of students before and after graduation. The dataset includes variables such as gender, race, socioeconomic status, and standardized test scores in reading, writing, math, science, and social studies. For this analysis, we focused on four predictors: socioeconomic status, type of school attended (public or private), type of academic program enrolled in (general, academic, or vocations), and reading scores, and one response variable, writing scores.\n\nhsb2=read.csv(\"hsb2.csv\")\nhead(hsb2)\n\n   id gender  race    ses schtyp       prog read write math science socst\n1  70   male white    low public    general   57    52   41      47    57\n2 121 female white middle public vocational   68    59   53      63    61\n3  86   male white   high public    general   44    33   54      58    31\n4 141   male white   high public vocational   63    44   47      53    56\n5 172   male white middle public   academic   47    52   57      53    61\n6 113   male white middle public   academic   44    52   51      63    61\n\n\n\n\nData cleaning was the first step of our study. All variable names were easy to use incode and all quantitative variables had the correct type, thus none of these variables needed to be changed. However, when we checked the variable type for our categorical variables, we saw they were type ‘character’; we then changed them to type factor.\n\n#Checking our quantitative variable types\nsapply(hsb2[,c(\"read\",\"write\")], class)\n\n     read     write \n\"integer\" \"integer\" \n\n#Checking our cateforial variable types\nsapply(hsb2[,c (\"ses\",\"schtyp\",\"prog\")], class)\n\n        ses      schtyp        prog \n\"character\" \"character\" \"character\" \n\n#Changing them to type factor\nhsb2$ses = as.factor(hsb2$ses)\nhsb2$schtyp = as.factor(hsb2$schtyp)\nhsb2$prog = as.factor(hsb2$prog )\nhsb2$gender=as.factor(hsb2$gender)\n\nIn order to look for missing values, we counted the original number of rows in our dataset and then counted again after filtering all NAs from the data. We started off with 200 rows and after filtering the dataset, 200 rows remained, thus the data contained no missing values.\n\nnrow(hsb2) #checking our starting number of rows\n\n[1] 200\n\n# Removing missing data points\nhsb2= hsb2 %&gt;% filter(!is.na(read), !is.na(write),!is.na(ses),!is.na(schtyp),!is.na(prog))\nnrow(hsb2)\n\n[1] 200\n\nhsb2 %&gt;% ggplot(aes(x = read)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Reading Test Scores', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = write)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Writing Test Scores', y = 'Count')\n\n\n\n\n\n\n\n# Checking for any empty strings\nhsb2 %&gt;% ggplot(aes(x = ses)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Socioeconomic status', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = schtyp)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'School Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = prog)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\n\n\n\n\nNumerical Summaries\nOur research question we are trying to find is how are writing scores affected by multiple variables of the high school senior. The variables we are going to look at is reading scores, socioeconomic status, school type, and program type.\nLet’s first review our quantitative variables. For our standardized writing scores, our mean score is around 52.77 with range from 31 to 67. Our standard deviation is rounded to 9.5. For our standardized reading scores, our mean is 52.23, with range from 28 to 76. Our standard deviation for these scores is rounded to 10.3. We have 200 observations.\n\nsummary(hsb2$write)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  31.00   45.75   54.00   52.77   60.00   67.00 \n\nsd(hsb2$write)\n\n[1] 9.478586\n\nsummary(hsb2$read)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.00   44.00   50.00   52.23   60.00   76.00 \n\nsd(hsb2$read)\n\n[1] 10.25294\n\n\nIn terms of categorical variables, we can see that for socioeconomic status for students, the majority of them, 47.5%, are in the middle class. For school type, most of the senior students, 84%, are in public school. And finally for program type, most seniors, 52.5% are in academic program.\n\nhsb2 %&gt;% count(ses) %&gt;% mutate(percentage = n/sum(n)*100)\n\n     ses  n percentage\n1   high 58       29.0\n2    low 47       23.5\n3 middle 95       47.5\n\nhsb2 %&gt;% count(schtyp) %&gt;% mutate(percentage = n/sum(n)*100)\n\n   schtyp   n percentage\n1 private  32         16\n2  public 168         84\n\nhsb2 %&gt;% count(prog) %&gt;% mutate(percentage = n/sum(n)*100)\n\n        prog   n percentage\n1   academic 105       52.5\n2    general  45       22.5\n3 vocational  50       25.0\n\n\nVisual Summary\nWe have already seen some plots of our data in our data cleaning section but I will also be providing a bit more to visualize our data and the relationships between each other. Starting with our response variable, standardized writing scores.\n\nggplot(hsb2, aes(y=write)) + geom_boxplot(fill = \"lightblue\") + labs(title = \"Standardized Writing Scores\", y = \"Count\")+ theme_classic()\n\n\n\n\n\n\n\n\nEarlier in the data cleaning, we showed data visualization for each variable, now let us compare them to our response variable.\n\nggplot(hsb2, aes(x = ses,y=write)) + geom_boxplot(fill = \"lightblue\") + labs( y= \"Writing Score\", x = \"Socioeconomic Status\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = schtyp,y= write)) + geom_boxplot(fill = \"lightblue\") + labs(y = \"Writing Score\", x = \"School Type\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = prog,y= write)) + geom_boxplot(fill = \"lightblue\") + labs(y = \"Writing Score\", x = \"Program Type\")+ theme_classic()"
  },
  {
    "objectID": "Test.html#methods",
    "href": "Test.html#methods",
    "title": "STAT 244-SC (Sp 25)",
    "section": "",
    "text": "The first statistical test we conducted was K-fold cross validation. Cross validation is a frequently used technique for evaluating the performance of predictive models by assessing how well they are able to generalize to data that is independent of the training set. In K-fold cross validation, this is done through randomly partitioning the dataset into k approximately equal-sized groups or folds. The model is trained on k-1 folds and then tested on the remaining fold. This process is repeated k times so that each fold serves as the test set exactly one time. Mathematically, K-folds cross validation follows this procedure,\n\nDivide the data into k partitions of approximately equal size.\nRepeated the following for each group j (j=1,2,…,k)\nassign group j to the test set.\n\nFit a model on the remaining k-1 partitions or the training set.\nUse this model to predict the response for all observations in group j.\nCalculate the needed error metric for group j. In our case we used the MSE\n\nCombine this information to estimate model quality\n\n\\[CV_{(k)}=\\frac{1}{k}\\sum_{j=1}^{k}MSE_J\\]\nIn our study, we use the MSE as our error metric. The MSE penalizes larger errors more heavily than other metrics. This is important for our data because we are predicting test scores and thus making large errors in our prediction can have large impacts. One large disadvantage of using the MSE would be that it is sensitive to outliers. Since there may be some students whose test scores are considered outliers, this may influence the MSE more heavily than other metrics.\n\\(MSE=\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=(RMSE)^2\\)\nThe goal of cross validation is to estimate how well a model will perform on independent datasets to prevent overfitting. Overfitting can cause harmful biases that may be causing patterns in the training data to be generalized to future data, ultimately further perpetuating inaccurate, and sometimes dangerous information. This is especially important when conducting educational research as overfitting can lead to the spread of misleading and sometimes dangerous information that could impact a large proportion of the younger population. Cross validation provides more fair and accurate prediction of the population of interest.\nIn our study, we analysed the CV MSE of the following models using three different values of k. The models were as follows,\nModel 1: ‘write’ ~ ‘gender’ + ‘ses’ + ‘schtyp’ + ‘prog’ + ‘read’ + ’ math’ + ‘science’ + ‘socst’\nModel 2: ‘write’ ~ ‘ses’ + ’ schtyp’ + ‘prog’ + ‘read’\nFirst, we implemented a 10-fold cross validation,\n\nset.seed(24667)\nlm_spec=linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"lm\")\n\n# Model Estimation\nmodel1=lm_spec %&gt;%\n  fit(write~gender+ses+schtyp+prog+read+math+science+socst, data=hsb2)\n\nmodel2=lm_spec %&gt;%\n  fit(write~ses+schtyp+prog+read, data=hsb2)\n\n\nset.seed(9359)\nmodel1_cv=lm_spec %&gt;%\n  fit_resamples(\n    write~gender+ses+schtyp+prog+read+math+science+socst,\n    resamples=vfold_cv(hsb2,v=10),\n    metrics=metric_set(mae, rmse, rsq)\n  )\n\n\nmodel2_cv=lm_spec %&gt;%\n  fit_resamples(\n    write~ses+schtyp+prog+read,\n    resamples=vfold_cv(hsb2,v=10),\n    metrics=metric_set(mae,rmse,rsq)\n  )\n\n\nIn order to retrieve the MSE for all folds in our cross validation we must first unnest the cross validation to obtain each individual fold and the RMSE. We then squared the RMSE to obtain the MSE for each individual fold, and following the k-fold cross validation procedure, averaged the MSE for each model. These MSE values are what is printed below.\n\n\n# Get fold by fold and pull out the rmse for each. \nm1_rmse=model1_cv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\nm2_rmse=model2_cv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\n# Sqaure the found RMSE values\nm1_mse=m1_rmse$.estimate^2\n\nm2_mse=m2_rmse$.estimate^2\n\n# Average the MSE values across the 10 folds for each model\nm1_cv_mse=(1/10)*sum(m1_mse)\nm2_cv_mse=(1/10)*sum(m2_mse)\n\n# Print the CV MSE for both models. \nm1_cv_mse\n\n[1] 40.57728\n\nm2_cv_mse\n\n[1] 58.62453\n\n\n\n# Summarize RMSE for model 1\nm1_summary &lt;- m1_rmse %&gt;%\n  summarise(\n    model = \"Model 1\",\n    mean_rmse = mean(.estimate),\n    sd_rmse = sd(.estimate),\n    mean_mse = mean(.estimate^2),\n    sd_mse = sd(.estimate^2)\n  )\n\n# Summarize RMSE for model 2\nm2_summary &lt;- m2_rmse %&gt;%\n  summarise(\n    model = \"Model 2\",\n    mean_rmse = mean(.estimate),\n    sd_rmse = sd(.estimate),\n    mean_mse = mean(.estimate^2),\n    sd_mse = sd(.estimate^2)\n  )\n\n# Combine into one summary table\nmodel_metrics_summary &lt;- bind_rows(m1_summary, m2_summary)\n\n# Display the table\nmodel_metrics_summary\n\n# A tibble: 2 × 5\n  model   mean_rmse sd_rmse mean_mse sd_mse\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Model 1      6.30   0.986     40.6   12.2\n2 Model 2      7.60   0.968     58.6   14.9\n\n\n\nWe found that the CV MSE for model 1 is, 40.577 while the CV MSE for model 2 is 58.625.\n\nNext, we conducted a leave one out cross validation of the two models.\n\nset.seed(1376987)\n\nk=nrow(hsb2)-1\n\n# Performing K=199 CV\nmodel1_loocv = lm_spec %&gt;%\n  fit_resamples(\n    write ~ gender + ses + schtyp + prog + read + math + science + socst,\n    resamples = vfold_cv(hsb2,v=k),\n    metrics = metric_set(mae, rmse)\n  )\n\nmodel2_loocv = lm_spec %&gt;%\n  fit_resamples(\n    write ~ ses + schtyp + prog + read,\n    resamples = vfold_cv(hsb2,v=k),\n    metrics = metric_set(mae, rmse)\n  )\n\n# Calculated the CV MSE for model 1 \n\nm1_loo_rmse=model1_loocv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\nm1_loo_mse=m1_loo_rmse$.estimate^2\nm1_loo_mse=(1/k)*sum(m1_loo_mse)\n\n\n# Calulate the CV MSE for model 2\n\nm2_loo_rmse=model2_loocv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n \nm2_loo_mse=m2_loo_rmse$.estimate^2\nm2_loo_mse=(1/k)*sum(m2_loo_mse)\n\n\n# Print the CV MSE for both models. \n(m1_loo_mse)\n\n[1] 39.32325\n\n(m2_loo_mse)\n\n[1] 57.82414\n\n\n\nWe can see that the 199-Fold CV MSE for Model 1 is 39.323, and the 199-fold CV MSE for Model 2 is 57.824. This shows that Model 1 has the lowest CV MSE when k=199.\n\nFinally, we implemented a 75-fold Cross Validation. Since we tested K=10 and K=199, this provides information into the CV-MSE when k is between the two previously found values.\n\nset.seed(239762)\n# Performing k=75 CV on both models\n\nmodel1_75 = lm_spec %&gt;%\n  fit_resamples(\n    write ~ gender + ses + schtyp + prog + read + math + science + socst,\n    resamples = vfold_cv(hsb2,v=75),\n    metrics = metric_set(mae, rmse, rsq)\n  )\n\nmodel2_75 = lm_spec %&gt;%\n  fit_resamples(\n    write ~ ses + schtyp + prog + read,\n    resamples = vfold_cv(hsb2,v=75),\n    metrics = metric_set(mae, rmse, rsq)\n  )\n\n# Calculated the CV MSE for model 1 \n\nm1_75_rmse=model1_75 %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\nm1_75_mse=m1_75_rmse$.estimate^2\nm1_75_mse=(1/75)*sum(m1_75_mse)\n\n\n# Calulate the CV MSE for model 2\n\nm2_75_rmse=model2_75 %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n \nm2_75_mse=m2_75_rmse$.estimate^2\nm2_75_mse=(1/75)*sum(m2_75_mse)\n\n\n# Print the CV MSE for both models. \n(m1_75_mse)\n\n[1] 39.88911\n\n(m2_75_mse)\n\n[1] 57.52963\n\n\n\nWe find that the 75-Fold CV MSE for model 1 is, 39.889, while the 75-Fold CV MSE for model 2 is 57.530.\n\nNotice that for Model 1, when k=199, the CV error is the smallest. For model 2, we notice that the CV error is smallest for k=75. Both models seem to perform better when K is larger than 10. Based on the all three calculated CV tests, we see a clearly that the CV MSE for Model 1 is consistently lower than that of Model 2. Due to this persistent results, we will select Model 1 as our final model.\nModel 1: ‘write’ ~ ‘gender’ + ‘ses’ + ‘schtyp’ + ‘prog’ + ‘read’ + ’ math’ + ‘science’ + ‘socst’\n\n\n\nNow, we used backward stepwise selection to select our model and which variables are efficient for us. We build a model with all p possible predictors and then we repeat the following until only one predictor remains:\n\nRemove the 1 predictor that increases the MSE/MAE by the least\nBuild a model with the remaining predictors.\n\nWe then pick the best model. This method helps us balance accuracy and ensuring we avoid overfitting.\n\n#Model\nlm_spec = linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% set_mode(\"regression\")\n#Full Model \nlm_spec %&gt;% fit(write ~ gender + ses + schtyp + prog + read + math + science + socst, data = hsb2) %&gt;% tidy() %&gt;% mutate (p.value = round(p.value, 4))\n\n# A tibble: 11 × 5\n   term           estimate std.error statistic p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      15.0      3.85       3.89   0.0001\n 2 gendermale       -5.37     0.889     -6.04   0     \n 3 seslow            0.851    1.31       0.648  0.518 \n 4 sesmiddle        -0.149    1.06      -0.141  0.888 \n 5 schtyppublic     -1.07     1.22      -0.877  0.382 \n 6 proggeneral      -1.07     1.19      -0.896  0.371 \n 7 progvocational   -1.74     1.27      -1.36   0.174 \n 8 read              0.114    0.0657     1.74   0.0841\n 9 math              0.207    0.0704     2.94   0.0037\n10 science           0.260    0.0628     4.14   0.0001\n11 socst             0.216    0.0556     3.90   0.0001\n\n#Model Selection \nmodel_8predictors = lm_spec %&gt;% fit_resamples(write~ gender + ses + schtyp + prog + read + math + science + socst, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_7predictors = lm_spec %&gt;% fit_resamples(write~ gender + ses + schtyp + prog + read + math + socst, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_6predictors = lm_spec %&gt;% fit_resamples(write~ ses + schtyp + prog + read + math + socst, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_5predictors = lm_spec %&gt;% fit_resamples(write ~ ses + schtyp + prog + read + math, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_4predictors = lm_spec %&gt;% fit_resamples(write ~ ses + schtyp + prog + read, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_3predictors = lm_spec %&gt;% fit_resamples(write ~ ses + prog + read, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_2predictors = lm_spec %&gt;% fit_resamples(write ~ ses + read, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\nmodel_1predictors = lm_spec %&gt;% fit_resamples(write ~ read, resamples = vfold_cv(hsb2, v = 10), metrics = metric_set(mae))\n\n#Results\nresults = bind_rows( model_1predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 1),model_2predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 2),model_3predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 3),model_4predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 4),model_5predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 5),model_6predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 6),model_7predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 7),model_8predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 8))\n\n#Plot them out\nggplot(results %&gt;% filter(.metric == \"mae\") , aes(x = predictors, y = mean)) + geom_point(color = \"lightblue\") + geom_line(color = \"lightblue\") + theme_classic()+ labs(title = \"MAE vs Num of Predictors\", x =\" Num of Predictors\", y=\"MAE\")\n\n\n\n\n\n\n\n\nBased on our graph, we can see that our lowest MAE is with all eight of our predictors. As we continued to remove a predictor each time, our MAE increased which means that all variable are significant and contribute to our model and response variable."
  },
  {
    "objectID": "Test.html#conclusion",
    "href": "Test.html#conclusion",
    "title": "STAT 244-SC (Sp 25)",
    "section": "",
    "text": "After our K-Fold Cross-Validation method and backward stepwise selection method, they gave us the both results and we determined that Model 1 is the best. Model 1, includes all eight predictors (gender, ses, schtyp, prog, read, math, science, socst) had the lowest MAE, mean absolute error, with both methods.\nModel 1 is able to provided the most significant model for standardized writing scores as it shows a comprehensive view of the student’s academic and demographic statistics. They all contribute to a students’ writing scores which is what we are testing. We originally picked the four predictive variables (ses, schtyp, prog, and read) because we believed those were the only significant ones.\nHowever, through statistical analysis, we saw that additional variables (math, science, gender) help explain variance in the standardized writing scores. To conclude, through analysis we cannot accurately predict the student’s writing performance by only a subset of variables. Based on our methods, we need all variables which lets us know the students’ full demographics and academic statistics, which helps us accurately predict student’s writing scores to the best we can."
  },
  {
    "objectID": "Test.html#source",
    "href": "Test.html#source",
    "title": "STAT 244-SC (Sp 25)",
    "section": "",
    "text": "National Center for Education Statistics - Statistical Consulting. (n.d.). High School & Beyond (HS&B) - overview. High School & Beyond (HS&B) - Overview. https://nces.ed.gov/surveys/hsb/\nExploring Data with Graphic; R Learning Modules. UCLA: Statistical Consulting Group. from https://stats.oarc.ucla.edu/r/modules/exploring-data-with-graphics/ (accessed April 28, 2025)."
  },
  {
    "objectID": "FP CP3_Aho, Garcia.html",
    "href": "FP CP3_Aho, Garcia.html",
    "title": "Final Project CP3",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(readxl)\nlibrary(tidymodels)\nhsb2=read.csv(\"hsb2.csv\")\nhsb2$ses = as.factor(hsb2$ses)\nhsb2$schtyp = as.factor(hsb2$schtyp)\nhsb2$prog = as.factor(hsb2$prog )\nhsb2$gender= as.factor(hsb2$gender)"
  },
  {
    "objectID": "FP CP3_Aho, Garcia.html#cross-validation",
    "href": "FP CP3_Aho, Garcia.html#cross-validation",
    "title": "Final Project CP3",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nWrite a brief introduction to cross validation which includes relevant mathematical notation.\n\n\nCross validation is a resampling method that is used to evaluate how well a model can generalize by splitting the data into test and training sets. The training data is used to fit a predicitive model. The model is then used on the test set and the model’s performance is evaluated.\nMathematically, for k-fold cross validation:\n\nDivide the data into k partitions of approximately equal size.\nRepeated the following for each group j (j=1,2,…,k)\n\nassign group j to the test set.\nFit a model on the remaining k-1 partitions or the training set.\nUse this model to predict the response for all observations in group j.\nCalculate the needed error metric for group j. In our case we used the MSE\n\nCombine this information to estimate model quality\n\\[\nCV_(k)=\\frac{1}{k}\\sum_{j=1}^{k}MSE_J\n\\]\n\n\n\nWhat is the goal of cross-validation? (Hint: Think about over-fitting, along with social/ethical considerations)\n\nThe goal of cross-validation is to estimate how well a model will perform on independent datasets. Cross validations helps avoid overfitting which can limit the harmful biases that cause patterns in the data that are then perpetuated due to poor generalization by the model. Cross-validation provides more fair and accurate predictions of the population of interest.\n\nWhat linear models are you considering based on your research question? Pick at least two models to compare.\n\n\nWe will compare the following two models\nModel 1: ‘write’ ~ ‘gender’ + ‘ses’ + ‘schtyp’ + ‘prog’ + ‘read’ + ’ math’ + ‘science’ + ‘socst’\nModel 2: ‘write’ ~ ‘ses’ + ’ schtyp’ + ‘prog’ + ‘read’\n\n\nset.seed(24667)\nlm_spec=linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"lm\")\n\n# Model Estimation\nmodel1=lm_spec %&gt;%\n  fit(write~gender+ses+schtyp+prog+read+math+science+socst, data=hsb2)\n\nmodel2=lm_spec %&gt;%\n  fit(write~ses+schtyp+prog+read, data=hsb2)\n\n\nExplain how you divided your data into its test set and training set.\n\n\nWe will divide the data into its test set and training set by first, splitting the data into 10 folds. Then one fold is assigned to the test set and the remaining 9 are assigned to the training set. The fold acting as the test dataset is changed each time the process is repeated.\n\n\nState which error metric you are using (MAE or MSE) and give its formal mathematical definition. Why did you choose this error metric? What are the advantages/disadvantages of using it?\n\n\nWe will use the MAE as our error metric.\n\n\\(MSE=\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=(RMSE)^2\\)\n\nThe MSE penalizes larger errors more heavily than other metrics. This is important for our data because we are predicting test scores and thus making large errors in our prediction can have large impacts. One large disadvantage of using the MSE would be that it is sensitive to outliers. Since there may be some students whose test scores are considered outliers, this may influence the MSE more heavily than other metrics.\n\n\nImplement k-fold cross validation for k = 10.\n\n\nFirst we perform a 10-fold cross validation on moth models found on part 3.\n\n\nset.seed(9359)\nmodel1_cv=lm_spec %&gt;%\n  fit_resamples(\n    write~gender+ses+schtyp+prog+read+math+science+socst,\n    resamples=vfold_cv(hsb2,v=10),\n    metrics=metric_set(mae, rmse, rsq)\n  )\n\n\nmodel2_cv=lm_spec %&gt;%\n  fit_resamples(\n    write~ses+schtyp+prog+read,\n    resamples=vfold_cv(hsb2,v=10),\n    metrics=metric_set(mae,rmse,rsq)\n  )\n\n\nIn order to retrieve the MSE for all folds in our cross validation we must first unnest the cross validation to obtain each individual fold and the RMSE. We then squared the RMSE to obtain the MSE for each individual fold, and following the k-fold cross validation procedure, averaged the MSE for each model. These MSE values are what is printed below.\n\n\n# Get fold by fold and pull out the rmse for each. \nm1_rmse=model1_cv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\nm2_rmse=model2_cv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\n# Sqaure the found RMSE values\nm1_mse=m1_rmse$.estimate^2\n\nm2_mse=m2_rmse$.estimate^2\n\n# Average the MSE values across the 10 folds for each model\nm1_cv_mse=(1/10)*sum(m1_mse)\nm2_cv_mse=(1/10)*sum(m2_mse)\n\n# Print the CV MSE for both models. \nm1_cv_mse\n\n[1] 40.57728\n\nm2_cv_mse\n\n[1] 58.62453\n\n\n\nWe found:\nThe CV MSE for model 1 is, 40.577.\nThe CV MSE for model 2 is 58.625\n\n\nDisplay evaluation metrics for your different models in a clean, organized way. This display should include both the estimated CV metric as well as its standard deviation.\n\n\n# Summarize RMSE for model 1\nm1_summary &lt;- m1_rmse %&gt;%\n  summarise(\n    model = \"Model 1\",\n    mean_rmse = mean(.estimate),\n    sd_rmse = sd(.estimate),\n    mean_mse = mean(.estimate^2),\n    sd_mse = sd(.estimate^2)\n  )\n\n# Summarize RMSE for model 2\nm2_summary &lt;- m2_rmse %&gt;%\n  summarise(\n    model = \"Model 2\",\n    mean_rmse = mean(.estimate),\n    sd_rmse = sd(.estimate),\n    mean_mse = mean(.estimate^2),\n    sd_mse = sd(.estimate^2)\n  )\n\n# Combine into one summary table\nmodel_metrics_summary &lt;- bind_rows(m1_summary, m2_summary)\n\n# Display the table\nmodel_metrics_summary\n\n# A tibble: 2 × 5\n  model   mean_rmse sd_rmse mean_mse sd_mse\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Model 1      6.30   0.986     40.6   12.2\n2 Model 2      7.60   0.968     58.6   14.9\n\n\n\nTry different values of k (the tuning parameter). At minimum, try k = n - 1 (LOOCV), and k = 5. Which value of k has the smallest CV error?\n\n\n(n-1)-fold Cross Fold Validation\n\n\nset.seed(1376987)\n\nk=nrow(hsb2)-1\n\n# Performing K=199 CV\nmodel1_loocv = lm_spec %&gt;%\n  fit_resamples(\n    write ~ gender + ses + schtyp + prog + read + math + science + socst,\n    resamples = vfold_cv(hsb2,v=k),\n    metrics = metric_set(mae, rmse)\n  )\n\nmodel2_loocv = lm_spec %&gt;%\n  fit_resamples(\n    write ~ ses + schtyp + prog + read,\n    resamples = vfold_cv(hsb2,v=k),\n    metrics = metric_set(mae, rmse)\n  )\n\n# Calculated the CV MSE for model 1 \n\nm1_loo_rmse=model1_loocv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\nm1_loo_mse=m1_loo_rmse$.estimate^2\nm1_loo_mse=(1/k)*sum(m1_loo_mse)\n\n\n# Calulate the CV MSE for model 2\n\nm2_loo_rmse=model2_loocv %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n \nm2_loo_mse=m2_loo_rmse$.estimate^2\nm2_loo_mse=(1/k)*sum(m2_loo_mse)\n\n\n# Print the CV MSE for both models. \n(m1_loo_mse)\n\n[1] 39.32325\n\n(m2_loo_mse)\n\n[1] 57.82414\n\n\n\nWe can see that the 199-Fold CV MSE for Model 1 is 39.323, and the 199-fold CV MSE for Model 2 is 57.824. This shows that Model 1 has the lowest CV MSE when k=199.\nNext, we tried a Cross Validation for K=75. Since we tested K=10 and K=199, we wanted to measure the MSE for when k is between the two previously found values.\n\n\nset.seed(239762)\n# Performing k=75 CV on both models\n\nmodel1_75 = lm_spec %&gt;%\n  fit_resamples(\n    write ~ gender + ses + schtyp + prog + read + math + science + socst,\n    resamples = vfold_cv(hsb2,v=75),\n    metrics = metric_set(mae, rmse, rsq)\n  )\n\nmodel2_75 = lm_spec %&gt;%\n  fit_resamples(\n    write ~ ses + schtyp + prog + read,\n    resamples = vfold_cv(hsb2,v=75),\n    metrics = metric_set(mae, rmse, rsq)\n  )\n\n! Fold51: internal: A correlation computation is required, but `truth` is constant and has 0...\n\n\n! Fold58: internal: A correlation computation is required, but `truth` is constant and has 0...\n\n# Calculated the CV MSE for model 1 \n\nm1_75_rmse=model1_75 %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n\nm1_75_mse=m1_75_rmse$.estimate^2\nm1_75_mse=(1/75)*sum(m1_75_mse)\n\n\n# Calulate the CV MSE for model 2\n\nm2_75_rmse=model2_75 %&gt;% unnest(.metrics) %&gt;%\n  filter(.metric==\"rmse\")\n \nm2_75_mse=m2_75_rmse$.estimate^2\nm2_75_mse=(1/75)*sum(m2_75_mse)\n\n\n# Print the CV MSE for both models. \n(m1_75_mse)\n\n[1] 39.88911\n\n(m2_75_mse)\n\n[1] 57.52963\n\n\n\nWe find that the 75-Fold CV MSE for model 1 is, 39.889, while the 75-Fold CV MSE for model 2 is 57.530.\nWe notice that for Model 1, when k=199, the CV error is the smallest. For model 2, we notice that the CV error is smallest for k=75. Both models seem to perform better when K is larger than 10.\n\n\nSelect your final model based on which one has the smallest CV error.\n\n\nBased on the all three calculated CV tests, we see a clearly that the CV MSE for Model 1 is consistantly lower than that of Model 2. Due to this persistent results, we will select Model 1 as our final model.\nModel 1: ‘write’ ~ ‘gender’ + ‘ses’ + ‘schtyp’ + ‘prog’ + ‘read’ + ’ math’ + ‘science’ + ‘socst’"
  },
  {
    "objectID": "FP CP3_Aho, Garcia.html#variable-subset-selection",
    "href": "FP CP3_Aho, Garcia.html#variable-subset-selection",
    "title": "Final Project CP3",
    "section": "Variable Subset Selection",
    "text": "Variable Subset Selection\n\nIdentify at least p = 3 predictors for modeling the expected response E(Y) of one of your variables Y.\n\nWe are starting with 4 predictors which are read (standardized reading score), ses (socioeconomic status), schtyp (school type), and prog (program type). Our p =4! Our response variable is write (standardized writing score).\n\nState which error metric you are using (i.e., CV MAE or CV MSE and state the chosen k-value).\n\nWe will use Cross Validation MAE with k = 5.\n\nBest subset selection: Only implement this if you have under 100 rows/observations in your data set. Otherwise, skip this! It will be too computationally expensive!\n\nBuild all 2p possible models that use any combination of the available predictors. Which model do you pick?\n\nSkipping since our data has 200 observations!\n\nBackward subset selection: Implement backward subset selection by starting with at least p = 3 predictors.\n\nWe start with p = 4 predictors and we perform backward subset selection. We can see that the one with the lowest MAE is with all the predictors.\n\n\n\nlm_spec = linear_reg()%&gt;% set_engine(\"lm\") %&gt;% set_mode(\"regression\")\n\nmodel_4predictors = lm_spec %&gt;% fit_resamples(write~ read + ses + schtyp + prog, resamples = vfold_cv(hsb2, v = 5), metrics = metric_set(mae) )\nmodel_4predictors %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    6.21     5   0.299 Preprocessor1_Model1\n\nmodel_3predictors = lm_spec %&gt;% fit_resamples(write~ read + ses + schtyp , resamples = vfold_cv(hsb2, v = 5), metrics = metric_set(mae) )\nmodel_3predictors %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    6.31     5   0.310 Preprocessor1_Model1\n\nmodel_2predictors = lm_spec %&gt;% fit_resamples(write~ read + ses , resamples = vfold_cv(hsb2, v = 5), metrics = metric_set(mae) )\nmodel_2predictors %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    6.37     5   0.208 Preprocessor1_Model1\n\nmodel_1predictors = lm_spec %&gt;% fit_resamples(write~ read , resamples = vfold_cv(hsb2, v = 5), metrics = metric_set(mae) )\nmodel_1predictors %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    6.41     5   0.239 Preprocessor1_Model1\n\n\n\nWe have to pick a value for our tuning parameter (the number of predictors). Plot your cross validated error as a function of the number of predictors, where each model has predictors chosen based on your backward subset selection.\n\nAs we can see again, our lowest MAE is from our model with all four predictors\n\n\n\nresults = bind_rows( model_1predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 1),model_2predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 2),model_3predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 3),model_4predictors %&gt;% collect_metrics()%&gt;% mutate(predictors = 4))\n\nggplot(results , aes(x = predictors, y = mean)) + geom_point(color = \"lightblue\") + geom_line(color = \"lightblue\") + theme_classic()\n\n\n\n\n\n\n\n\n\nBased on your plot, which model do you pick?\n\nWe pick our model with all four predictors.\n\nWhy is backward subset selection a greedy algorithm? Answer in 1 - 3 sentences.\n\nBackwards subset selection is a greedy algorithm because it removes one predictor each time and uses computer runtime for it. This also means that it removes previous predictors, and often misses the most optimal predictor combination."
  },
  {
    "objectID": "FP CP3_Aho, Garcia.html#source",
    "href": "FP CP3_Aho, Garcia.html#source",
    "title": "Final Project CP3",
    "section": "Source:",
    "text": "Source:\nNational Center for Education Statistics - Statistical Consulting. (n.d.). High School & Beyond (HS&B) - overview. High School & Beyond (HS&B) - Overview. https://nces.ed.gov/surveys/hsb/\nExploring Data with Graphic; R Learning Modules. UCLA: Statistical Consulting Group. from https://stats.oarc.ucla.edu/r/modules/exploring-data-with-graphics/ (accessed April 28, 2025)."
  },
  {
    "objectID": "FP CP2.html",
    "href": "FP CP2.html",
    "title": "Final Project CP2",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2"
  },
  {
    "objectID": "FP CP2.html#part-1-data-context",
    "href": "FP CP2.html#part-1-data-context",
    "title": "Final Project CP2",
    "section": "Part 1: Data Context",
    "text": "Part 1: Data Context\n\nWhat variables in your data set are you interested in? Are they quantitative or categorical?\nIf quantitative, provide the units. If categorical, list or describe the categories.\n\n\nWe are interested in the following variables.\n\n‘write’: Standardized writing score from various tests; measured in standard errors.\n‘read’: standardized reading scores from various tests; measured in standard errors.\n‘ses’: Socioeconomic status of student’s family. Categories: low, middle, high\n‘schtyp’: Type of school attending. Categories: public, private\n‘prog’: Type of program enrolled in. Categories: general, academic, vocational\n\n\n\nWhat does one observational unit (row) represent in your data set?\n\n\nOne observational unit in our data set represents one High School student who participated in the National Center of Education Statistics survey titled “High School and Beyond”.\n\n\nHow was the sample obtained? If you can find what the sampling method was, include that information. Also include what year the data was collected\n\n\nThe data was obtained through surveys of 1,100 10th-12th graders in 1980 conducted by the National Center for Education Statistics. The data we are using in our project is a random sample of size 200 taken from the data in 2016 by UCLA Institute for Digital Research & Education Statistical Consulting. There are no specifics of how this sample was taken.\n\n\nCould the data collectors have an ulterior motive for collecting the data (e.g., solicitation of private information)? Could the data collectors have an ulterior motive for collecting a biased sample, or otherwise misrepresenting the population in any way (e.g., trying to reinforce a predetermined narrative)?\n\n\nThere is no clear ulterior motive for collecting this data. Although standardized test scores may influence government funding for particular districts and could pose as an ulterior motive for collecting the data, this sample of 200 students is too small to have an influence on any particular school district. There is also no data collected in the sample on where students attend school.\n\n\nDo you think the source of your data is reliable? Do you trust how the data was collected?\n\nYes, we believe the source of our data is reliable. It was collected by the “federal statistical agency responsible for collecting, analyzing, and reporting data on the condition of U.S. education.” Our data was sampled by UCLA, which is a reliable academic source.\nThe data was collected via survey, thus ome survey bias may be present in the data. It should be noted that there is no clear indication of such bias."
  },
  {
    "objectID": "FP CP2.html#part-2-data-cleaning",
    "href": "FP CP2.html#part-2-data-cleaning",
    "title": "Final Project CP2",
    "section": "Part 2: Data Cleaning",
    "text": "Part 2: Data Cleaning\nLoading our data\n\nhsb2=read.csv(\"hsb2.csv\")\nhead(hsb2)\n\n   id gender  race    ses schtyp       prog read write math science socst\n1  70   male white    low public    general   57    52   41      47    57\n2 121 female white middle public vocational   68    59   53      63    61\n3  86   male white   high public    general   44    33   54      58    31\n4 141   male white   high public vocational   63    44   47      53    56\n5 172   male white middle public   academic   47    52   57      53    61\n6 113   male white middle public   academic   44    52   51      63    61\n\n\n\nDo the variables you listed in (1) have names that are easy to use in code? (Examples of names that might not be easy to work with are those with spaces, those that are very long, etc.). If not, use mutate() and select() to rename your variables (or the rename() function).\n\nYes! All of our values are simple and short enough that we do not need to change any of the names!\n\n\ncolnames(hsb2)\n\n [1] \"id\"      \"gender\"  \"race\"    \"ses\"     \"schtyp\"  \"prog\"    \"read\"   \n [8] \"write\"   \"math\"    \"science\" \"socst\"  \n\n# No need to improve variable nams as our variables names in hsb2 are simple enough\n\nDo your quantitative variables have the correct types (e.g., double for decimals, integer for whole numbers, etc.). If not, set them to be the correct type. You can check the type of a variable using the class() function.\n\n#Checking our quantitative variable types\nsapply(hsb2[,c(\"read\",\"write\")], class)\n\n     read     write \n\"integer\" \"integer\" \n\n\n\nAll our quantitative variables have type double, so we are all good!\n\nDo your categorical variables have the type factor? If not, give them this type to tell R that they are categorical.\n\n#Checking our cateforial variable types\nsapply(hsb2[,c (\"ses\",\"schtyp\",\"prog\")], class)\n\n        ses      schtyp        prog \n\"character\" \"character\" \"character\" \n\n#Changing them to type factor\nhsb2$ses = as.factor(hsb2$ses)\nhsb2$schtyp = as.factor(hsb2$schtyp)\nhsb2$prog = as.factor(hsb2$prog )\n\n\nWhen we checked our variable type for our categorical variables, we saw they were as type ‘character’. We then changed them to type factor!\n\nAre there any variables you need to create? (For example, you might want a variable that combines existing categories, a categorical version of a quantitative variable, etc.) \n\nNo there is no need to create new variables with the research we want from this data set!\n\nAre there any missing values for your variables listed in (9)? If so, filter them out. Note: Some data sets use empty strings “” to denote missing values, so check for these as well.\n\nnrow(hsb2) #checking our starting number of rows\n\n[1] 200\n\n# Removing missing data points\nhsb2= hsb2 %&gt;% filter(!is.na(read), !is.na(write),!is.na(ses),!is.na(schtyp),!is.na(prog))\nnrow(hsb2)\n\n[1] 200\n\nhsb2 %&gt;% ggplot(aes(x = read)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Reading Test Scores', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = write)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Standardized Writing Test Scores', y = 'Count')\n\n\n\n\n\n\n\n# Checking for any empty strings\nhsb2 %&gt;% ggplot(aes(x = ses)) +geom_bar(fill = \"lightblue\") +theme_classic()  +\nlabs(x = 'Socioeconomic status', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = schtyp)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'School Type', y = 'Count')\n\n\n\n\n\n\n\nhsb2 %&gt;% ggplot(aes(x = prog)) +geom_bar(fill = \"lightblue\") +theme_classic()+\nlabs(x = 'Program Type', y = 'Count')\n\n\n\n\n\n\n\n\n\nWe started off with 200 rows and then checked for any NA values, and we saw there was none to remove! We also used plots to check for any empty string values and we did not find any!\n\nHow many observational units (rows) do you have after the previous step?\n\nWe still have 200 rows after our previous step.\n\nDoes the source of data say that certain data points are missing? (For example, in NHANES we are told that the variable Height is measured only for participants aged 2 years or older, meaning Height will have NA values in rows corresponding to participants under 2.) If not available in the data description, why do you think some data points are missing? Is the mechanism for missingness completely random, or could there be something systematic which leads to greater rates of missingness? It’s okay if you’re not sure how to answer this question — just do your best to think about it.\n\nYes, our data randomly sampled 200 data points from High School and Beyond Survey from 2016. This data was collected of high school seniors so this does not include other grade levels. We were not able to find how the data was randomly sampled as well. From our previous step, we also did not find anything rows to remove.\n\n\n\n#Comparing multiple variables\nggpairs(hsb2 %&gt;% select(write,read,ses,schtyp,prog))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "FP CP2.html#part-3-exploratory-data-analysis",
    "href": "FP CP2.html#part-3-exploratory-data-analysis",
    "title": "Final Project CP2",
    "section": "Part 3: Exploratory Data Analysis",
    "text": "Part 3: Exploratory Data Analysis\n\nProvide any numerical summaries that are relevant to your research question. These are highly dependent on your question, but some ideas are:\n\n\nFor one quantitative variable, some relevant numerical summaries involve:\n\nOne measure of center (e.g., mean, median);\nOne measure of spread (e.g., standard deviation, variance, IQR, a minimum/maximum);\nAn overall count (how many observations do you have?)\n\nFor one categorical variable, a relevant numerical summary might be:\n\nthe # of observations in each category (i.e., use count());\nthe proportion (or percentage) of observations in each category.\n\n\n\nOur research question we are trying to find is how are writing scores affected by multiple variables of the highschool senior. The variables we are going to look at is reading scores, socioeconomic status, school type, and program type.\nLets first review our quantitative variables. For our standardized writing scores, our mean score is around 52.77 with range from 31 to 67. Our standard deviation is rounded to 9.5. For our standardized reading scores, our mean is 52.23, with range from 28 to 76. Our standard deviation for these scores is rounded to 10.3. We have 200 observations.\n\n\nsummary(hsb2$write)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  31.00   45.75   54.00   52.77   60.00   67.00 \n\nsd(hsb2$write)\n\n[1] 9.478586\n\nsummary(hsb2$read)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.00   44.00   50.00   52.23   60.00   76.00 \n\nsd(hsb2$read)\n\n[1] 10.25294\n\n\n\nNow our categorical variables!\nWe can see that for socioeconomic status for students, the majority of them, 47.5%, are in the middle class. For school type, most of the senior students, 84%, are in public school. And finally for program type, most seniors, 52.5% are in academic program.\n\n\nhsb2 %&gt;% count(ses) %&gt;% mutate(percentage = n/sum(n)*100)\n\n     ses  n percentage\n1   high 58       29.0\n2    low 47       23.5\n3 middle 95       47.5\n\nhsb2 %&gt;% count(schtyp) %&gt;% mutate(percentage = n/sum(n)*100)\n\n   schtyp   n percentage\n1 private  32         16\n2  public 168         84\n\nhsb2 %&gt;% count(prog) %&gt;% mutate(percentage = n/sum(n)*100)\n\n        prog   n percentage\n1   academic 105       52.5\n2    general  45       22.5\n3 vocational  50       25.0\n\n\n\nProvide any visualizations that are relevant to your research question. Chapter 2 in this online text is a good resource for creating different plots.\n\nThe choice of visualization can be highly dependent on your question, but some ideas are:\n\nA visual summary of your (quantitative) outcome of interest:\n\nExamples: a density plot, a box plot, a histogram, a relative frequency histogram, etc.\n\nA visual summary of your predictor(s) of interest:\n\nFor one quantitative predictor, the examples in the previous bullet point apply\nFor one categorical predictor, you could do a bar chart, mosaic plot, etc.\n\nA visual summary of the relationship between one quantitative variable and one or more different variables:\n\nIdeas: scatter plots (maybe with points colored by a category of some categorical variable), side-by-side histograms, side-by-side box plots, side-by-side histograms, overlaid density plots, etc.\n\n\n\nWe have already seen some plots of our data in our data cleaning section but I will also be providing a bit more to visualize our data and the relationships between each other. Starting with our response variable, standardized writing scores.\n\n\n\nggplot(hsb2, aes(y=write)) + geom_boxplot(fill = \"lightblue\") + labs(title = \"Standardized Writing Scores\", y = \"Count\")+ theme_classic()\n\n\n\n\n\n\n\n\n\nEarlier in the data cleaning, we showed data visualization for each variable, now let us compare them to our response variable.\n\n\nggplot(hsb2, aes(x = ses,y=write)) + geom_boxplot(fill = \"lightblue\") + labs( y= \"Writing Score\", x = \"Socioeconomic Status\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = schtyp,y= write)) + geom_boxplot(fill = \"lightblue\") + labs(y = \"Writing Score\", x = \"School Type\")+ theme_classic()\n\n\n\n\n\n\n\nggplot(hsb2, aes(x = prog,y= write)) + geom_boxplot(fill = \"lightblue\") + labs(y = \"Writing Score\", x = \"Program Type\")+ theme_classic()"
  },
  {
    "objectID": "FP CP2.html#source",
    "href": "FP CP2.html#source",
    "title": "Final Project CP2",
    "section": "Source:",
    "text": "Source:\nNational Center for Education Statistics - Statistical Consulting. (n.d.). High School & Beyond (HS&B) - overview. High School & Beyond (HS&B) - Overview. https://nces.ed.gov/surveys/hsb/\nExploring Data with Graphic; R Learning Modules. UCLA: Statistical Consulting Group. from https://stats.oarc.ucla.edu/r/modules/exploring-data-with-graphics/ (accessed April 28, 2025)."
  }
]